{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e7b3152-a6af-4dc6-976e-7a387816150e",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd7dd2-868e-4cc4-8494-e7d551f0ff32",
   "metadata": {},
   "source": [
    "Ridge Regression is a linear regression technique that adds a penalty term (L2 regularization) to the ordinary least squares (OLS) regression. Unlike OLS regression, which aims to minimize the sum of squared residuals, Ridge Regression also aims to minimize the sum of squared coefficients, subject to a constraint. This penalty term helps to reduce the impact of multicollinearity and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499f6a72-b0d9-4990-b385-b1ba6a24177b",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e0aa4-4d33-40a2-b9ad-7476c884a96a",
   "metadata": {},
   "source": [
    "The assumptions of Ridge Regression are similar to those of linear regression:\n",
    "\n",
    "Linearity: The relationship between the dependent variable and the independent variables is assumed to be linear.\n",
    "Independence: The observations are assumed to be independent of each other.\n",
    "Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables.\n",
    "Normality: The residuals are normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f408af-c72c-40fa-aa84-ca63f51a0a36",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aba12d-c477-4270-85e0-8e5ca4df62e4",
   "metadata": {},
   "source": [
    "The value of the tuning parameter (lambda or Î±) in Ridge Regression is typically selected using techniques such as cross-validation or grid search. These methods involve testing the model's performance on different values of lambda and selecting the one that yields the best balance between bias and variance. The choice of the tuning parameter depends on the specific problem and the trade-off between model complexity and model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cdeda-6b95-4d3e-81de-a4ba717d6ef8",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b775d1-c4ab-4d01-897a-b5b3ea014d05",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for feature selection. The penalty term in Ridge Regression has a shrinking effect on the coefficients, reducing their magnitude. As lambda increases, the impact of less important predictors decreases, and their coefficients may shrink towards zero. This allows Ridge Regression to perform implicit feature selection by assigning smaller coefficients to less relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f8fe4-b8eb-4a86-a83a-a2943daff79e",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4468fd34-9176-44cb-882f-2619023a9b2d",
   "metadata": {},
   "source": [
    "Ridge Regression performs well in the presence of multicollinearity. Multicollinearity occurs when independent variables are highly correlated with each other. In ordinary least squares regression, multicollinearity can lead to unstable and unreliable coefficient estimates. Ridge Regression addresses this issue by shrinking the coefficients and reducing their sensitivity to collinearity. The penalty term in Ridge Regression helps stabilize the estimates and produces more robust predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e961ed-249f-4e48-a488-6acb28b02f8b",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbf771-dccb-4410-9480-4a39c29073de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4202818-f335-4268-bd52-ce6446e8488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654a1849-53ed-42f1-8115-15a59e22d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a24bc-948c-4fd8-a76c-7234ed8e9832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
